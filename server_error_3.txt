warning: C:\Users\Trinity\Documents\daydream\Ask_Pete\crates\ask_pete_client\Cargo.toml: unused manifest key: dependencies.tokio.getrandom
    Checking bevy_window v0.13.2
    Checking bevy_audio v0.13.2
    Checking bevy_scene v0.13.2
    Checking ask_pete_ai v0.1.0 (C:\Users\Trinity\Documents\daydream\Ask_Pete\crates\ask_pete_ai)
error[E0583]: file not found for module `weigh_station`
  --> crates\ask_pete_ai\src\lib.rs:10:1
   |
10 | pub mod weigh_station;
   | ^^^^^^^^^^^^^^^^^^^^^^
   |
   = help: to create the module `weigh_station`, create file "crates\ask_pete_ai\src\weigh_station.rs" or "crates\ask_pete_ai\src\weigh_station\mod.rs"
   = note: if there is a `mod weigh_station` elsewhere in the crate already, import it with `use crate::...` instead

error[E0432]: unresolved imports `local_inference::LocalConfigWrapper`, `local_inference::LocalModel`
  --> crates\ask_pete_ai\src\lib.rs:12:27
   |
12 | pub use local_inference::{LocalConfigWrapper, LocalModel};
   |                           ^^^^^^^^^^^^^^^^^^  ^^^^^^^^^^ no `LocalModel` in `local_inference`
   |                           |
   |                           no `LocalConfigWrapper` in `local_inference`
   |                           help: a similar name exists in the module: `GemmaConfigWrapper`

error[E0433]: failed to resolve: could not find `LlamaEosToks` in `llama`
   --> crates\ask_pete_ai\src\llm\llama_engine.rs:122:68
    |
122 |             eos_token_id: Some(candle_transformers::models::llama::LlamaEosToks::Single(
    |                                                                    ^^^^^^^^^^^^ could not find `LlamaEosToks` in `llama`

error[E0412]: cannot find type `LocalModel` in module `crate::local_inference`
  --> crates\ask_pete_ai\src\socratic_engine.rs:30:49
   |
30 |     local_model: Option<crate::local_inference::LocalModel>,
   |                                                 ^^^^^^^^^^ not found in `crate::local_inference`

error[E0412]: cannot find type `LocalModel` in module `crate::local_inference`
  --> crates\ask_pete_ai\src\socratic_engine.rs:69:70
   |
69 |     pub fn set_local_model(&mut self, model: crate::local_inference::LocalModel) {
   |                                                                      ^^^^^^^^^^ not found in `crate::local_inference`

error[E0412]: cannot find type `CognitiveLoadProfile` in module `pete_core::physics`
   --> crates\ask_pete_ai\src\socratic_engine.rs:210:37
    |
210 |     ) -> Result<pete_core::physics::CognitiveLoadProfile> {
    |                                     ^^^^^^^^^^^^^^^^^^^^ not found in `pete_core::physics`

error[E0422]: cannot find struct, variant or union type `CognitiveLoadProfile` in module `pete_core::physics`
   --> crates\ask_pete_ai\src\socratic_engine.rs:216:36
    |
216 |             Ok(pete_core::physics::CognitiveLoadProfile {
    |                                    ^^^^^^^^^^^^^^^^^^^^ not found in `pete_core::physics`

error[E0560]: struct `local_inference::GenerationConfig` has no field named `seed`
   --> crates\ask_pete_ai\src\socratic_engine.rs:148:17
    |
148 |                 seed: 42,
    |                 ^^^^ `local_inference::GenerationConfig` does not have this field
    |
    = note: all struct fields are already assigned

error[E0599]: the method `clone` exists for enum `std::option::Option<GeminiClient>`, but its trait bounds were not satisfied
   --> crates\ask_pete_ai\src\socratic_engine.rs:237:32
    |
237 |             self.gemini_client.clone(),
    |                                ^^^^^ method cannot be called on `std::option::Option<GeminiClient>` due to unsatisfied trait bounds
    |
   ::: crates\ask_pete_ai\src\llm\gemini_client.rs:83:1
    |
 83 | pub struct GeminiClient {
    | ----------------------- doesn't satisfy `GeminiClient: Clone`
    |
   ::: C:\Users\Trinity\.rustup\toolchains\stable-x86_64-pc-windows-msvc\lib/rustlib/src/rust\library\core\src\option.rs:594:1
    |
594 | pub enum Option<T> {
    | ------------------ doesn't satisfy `std::option::Option<GeminiClient>: Clone`
    |
    = note: the following trait bounds were not satisfied:
            `GeminiClient: Clone`
            which is required by `std::option::Option<GeminiClient>: Clone`
help: consider annotating `GeminiClient` with `#[derive(Clone)]`
   --> crates\ask_pete_ai\src\llm\gemini_client.rs:83:1
    |
 83 + #[derive(Clone)]
 84 | pub struct GeminiClient {
    |

error[E0061]: this function takes 1 argument but 3 arguments were supplied
   --> crates\ask_pete_ai\src\socratic_engine.rs:236:29
    |
236 |         let mut architect = crate::architect::CurriculumArchitect::new(
    |                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
237 |             self.gemini_client.clone(),
238 |             self.local_model.clone(),
    |             ------------------------ unexpected argument #2
239 |             weigh_station,
    |             ------------- unexpected argument #3
    |
note: associated function defined here
   --> crates\ask_pete_ai\src\architect.rs:26:12
    |
 26 |     pub fn new(gemini: GeminiClient) -> Self {
    |            ^^^
help: remove the extra arguments
    |
237 -             self.gemini_client.clone(),
238 -             self.local_model.clone(),
237 +             self.gemini_client.clone(),
    |

error[E0560]: struct `candle_transformers::models::llama::Config` has no field named `bos_token_id`
   --> crates\ask_pete_ai\src\llm\llama_engine.rs:121:13
    |
121 |             bos_token_id: Some(128000),
    |             ^^^^^^^^^^^^ `candle_transformers::models::llama::Config` does not have this field
    |
    = note: all struct fields are already assigned

error[E0560]: struct `candle_transformers::models::llama::Config` has no field named `eos_token_id`
   --> crates\ask_pete_ai\src\llm\llama_engine.rs:122:13
    |
122 |             eos_token_id: Some(candle_transformers::models::llama::LlamaEosToks::Single(
    |             ^^^^^^^^^^^^ `candle_transformers::models::llama::Config` does not have this field
    |
    = note: all struct fields are already assigned

error[E0560]: struct `candle_transformers::models::llama::Config` has no field named `rope_scaling`
   --> crates\ask_pete_ai\src\llm\llama_engine.rs:125:13
    |
125 |             rope_scaling: None,
    |             ^^^^^^^^^^^^ `candle_transformers::models::llama::Config` does not have this field
    |
    = note: all struct fields are already assigned

error[E0560]: struct `candle_transformers::models::llama::Config` has no field named `max_position_embeddings`
   --> crates\ask_pete_ai\src\llm\llama_engine.rs:126:13
    |
126 |             max_position_embeddings: 131072,
    |             ^^^^^^^^^^^^^^^^^^^^^^^ `candle_transformers::models::llama::Config` does not have this field
    |
    = note: all struct fields are already assigned

error[E0560]: struct `candle_transformers::models::llama::Config` has no field named `tie_word_embeddings`
   --> crates\ask_pete_ai\src\llm\llama_engine.rs:127:13
    |
127 |             tie_word_embeddings: true,
    |             ^^^^^^^^^^^^^^^^^^^ `candle_transformers::models::llama::Config` does not have this field
    |
    = note: all struct fields are already assigned

Some errors have detailed explanations: E0061, E0412, E0422, E0432, E0433, E0560, E0583, E0599.
For more information about an error, try `rustc --explain E0061`.
error: could not compile `ask_pete_ai` (lib) due to 15 previous errors
warning: build failed, waiting for other jobs to finish...
